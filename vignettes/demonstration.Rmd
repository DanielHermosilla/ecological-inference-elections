---
title: "Demonstration of the package usage"
author: "Daniel Hermosilla"
date: "`r Sys.Date()`"

vignette: >
  %\VignetteIndexEntry{Demonstration of the package usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

output:
  rmarkdown::html_vignette:
    toc: false
    number_sections: false
---

The Expectation-Maximization algorithm is a statistical framework that relies on maximizing the log-likelihood of a conditional probability distribution. This algorithm depends on the heterogeneity of the sample and, as with every statistical method, it becomes more efficient as the sample size increases.

# Running the Expectation-Maximization algorithm 
One of the most meaningful use cases relies on political elections. As of this package, it provides the dataset with the Chilean Presidential Election from 2021. Such results are divided among electoral districts, where it's known the ages from each voter. 

To set up the algorithm, you can load a sample using `get_XW_chile()`:

```{r}
library(fastei)

el_golf <- get_XW_chile(elect_district = "EL GOLF")

el_golf
```

As shown in the output, it returns an eim object that contains two matrices: the number of votes per candidate and the number of votes per group.

Running the algorithm is as simple as calling `run_em()`. The most efficient method tends to be "mult":

```{r}
results <- run_em(el_golf)
results$prob
```

Note that each row corresponds to the probability that a demographic group (g) voted for a candidate (c). It is worth noting how the results differ substantially across groups.

# Getting standard deviation estimates

Itâ€™s important to note that the initial estimates may not always be efficient in terms of their standard deviation. Fortunately, the package provides a `bootstrap()` function that allows you to estimate the variability of the results:
```{r}
results <- bootstrap(results, seed = 42, nboot = 30)
results$sd
```

This output is expected, given that the number of ballot boxes in "EL GOLF" is substantial. However, it can be insightful to analyze smaller districts, such as "NAVIDAD":

```{r}
navidad <- get_XW_chile(elect_district = "NAVIDAD")
navidad <- bootstrap(navidad, seed = 42, nboot = 30)
navidad$sd
```

In this case, the probability variations are noticeably more significant, likely due to the smaller sample size.

# Reducing Variance Through Group Aggregation

Given the issue of high variability in smaller districts, an interesting alternative is to merge some demographic groups while maximizing the log-likelihood, subject to a standard deviation constraint. One way to approach this is with a *"greedy"* strategy, evaluating up to $2^{G-1}$ group combinations.

To approximate such a solution, the package provides a heuristic:

```{r}
navidad_proxy <- get_agg_proxy(navidad, seed = 42)
navidad_proxy$group_agg
```

As shown in the output, the heuristic found a feasible configuration by merging groups 1 with 2, 5 with 6, and 7 with 8. We can evaluate the effectiveness of this grouping by comparing the mean standard deviation to the original formulation:

```{r}
mean(navidad$sd) - mean(navidad_proxy$sd)
```

The full greedy algorithm is also included in the package. In theory, it explores a large number of combinations and may require substantial computation time. However, using the "mult" method offers a fast and efficient workaround:

```{r}
navidad_opt <- get_agg_opt(navidad, seed = 42)
navidad_opt$group_agg
```

Notably, the optimal group merging differs only slightly from the heuristic result, suggesting that the approximation performs quite well in this case.

# Comparing results

An usual use case comes among the question of how much could two districts differ. For instance, we can compare the results of "APOQUINDO" and "EL GOLF", two districts that usually yield similar results:

```{r}
apoquindo <- get_XW_chile(elect_district = "APOQUINDO")

comparison <- welchtest(
    X = el_golf$X,
    W = el_golf$W,
    X2 = apoquindo$X,
    W2 = apoquindo$W,
    method = "mult",
    nboot = 30,
    seed = 42
)

nonsignificant <- comparison$pvals >= 0.05
nonsignificant
```

Hence, it can be seen that the group whom voting behaviour changes the most is group 1, which is the youngest group. This is a common pattern in Chilean elections, where younger voters tend to be more volatile in their voting behaviour.

# Simulating results

In this section, we will examine each method using simulated elections. The methods "mcmc" and "mvn_cdf" are particularly flexible, supporting additional parameters.

We can generate samples using the function `simulate_election()`:

```{r}
samples <- simulate_election(num_groups = 3, num_candidates = 3, num_ballots = 50, seed = 42)
object <- eim(samples$X, samples$W)
```

All the methods attempt to approximate the probability of a group voting for a candidate, given observed votes. Let $X_{bgc}$ represent the observed votes and $Y_{bgc}$ represent the votes for candidate $c$ from group $g$ in ballot box $b$. Define $q_{bgc} = \mathbb{P}(Y_{bgc}=1 \mid X_{bgc})$ as the probability of a specific group voting for a candidate given the observed result. The methods aim to approximate:

$$q_{bgc}=\frac{P(\mathbf{X_b}=\mathbf{x_b}\;\vert\;Y_{bgc}=1)\cdot p_{gc}}{\sum_{c'\in\mathcal{C}}P(\mathbf{X_b}=\mathbf{x_b}\;\vert\;Y_{bgc'}=1)\cdot p_{gc'}}$$


## Multinomial

The multinomial method is the most efficient in practice, approximating:

$$q\approx\frac{x_{bc}\cdot p_{gc}\cdot r_{bgc}^{-1}}{\sum_{c'\in\mathcal{C}}x_{bc'}\cdot p_{gc'}\cdot r_{bgc}^{-1}}$$

Where 
$$r_{bgc} :=\frac{\sum_{g'\in\mathcal{G}}w_{bg'}p_{g'c}-p_{gc}}{I_b-1}$$

This method has computational complexity $O(B\cdot G\cdot C)$.

```{r}
multinomial <- run_em(object, method = "mult")
multinomial_mae <- mean(abs(multinomial$prob - samples$real_prob))
multinomial_mae
multinomial$time
```

This produces a Mean Absolute Error (MAE) of approximately $1.64\%$.

## Multivariate PDF

It is not necessary to compute the full probability $q_{bgc}$; instead, it can be approximated by simplifying the integration over the hypercube $\mathcal{A}$ in both the numerator and denominator:

$$q_{bgc}\approx\frac{\phi_{bgc}(x)\cdot p_{gc}}{\sum_{c'\in\mathcal{C}}\phi_{bgc'}(x)\cdot p_{gc}}$$

This method is also time-efficient

```{r}
multivariate_pdf <- run_em(object, method = "mvn_pdf")
multivariate_pdf_mae <- mean(abs(multivariate_pdf$prob - samples$real_prob))
multivariate_pdf_mae
multivariate_pdf$time
```

Provides an Mean Absolute Error (MAE) of approximately $1.54\%$.

## Multivariate CDF

Aproximates the first and second moments of the $q_{bgc}$. Since the approximation is continuos, it defines an hypercube defined by:

$$\mathcal{A}_{bc}(\mathbf{x_b})=\prod^{C-1}_{c'=1}\left[x_{bc}-\frac{1}{2}-\mathbf{1}_{c'=c}, x_{bc}+\frac{1}{2}-\mathbf{1}_{c'=c_{c'=c}}\right]$$

Yielding the following approximation:

$$q_{bgc}\approx\frac{\int_{\mathcal{A}_{bc}(\mathbf{x}_b)}\phi_{bgc}(x)dx\cdot p_{gc}}{\sum_{c'\in\mathcal{C}}\int_{\mathcal{A}_{bc'}}\phi_{bgc'}(x)dx\cdot p_{gc'}}$$

Where $\phi_{bgc}(\cdot)$ corresponds to the Multivariate Normal PDF.


```{r}
multivariate_cdf <- run_em(object, method = "mvn_cdf", mc_error = 1e-8)
multivariate_cdf_mae <- mean(abs(multivariate_cdf$prob - samples$real_prob))
multivariate_cdf_mae
multivariate_cdf$time
```

Providing a MAE of $6.5\%$. Let's denote that, on average, it tends to be much smaller.

## MCMC

Since there are an exponential number of ways in which the samples can be assigned, it is possible to use importance sampling (IS). Let $\mathcal{S}_b$ be the set of samples, then:

$$q_{bgc}\approx\frac{\sum_{\mathbf{z}_b\in\mathcal{S}_b}\frac{z_{bgc}}{w_{bg}}\prod_{g'\in\mathcal{G}}\left(\binom{w_{bg'}{z_{bg'1},\dots,z_{bg'C}}}\prod_{c'\in\mathcal{C}}p_{g'c'}^{z_{bg'c'}}\right)}{\sum_{\mathbf{z}_b\in\mathcal{S}_b}\prod_{g'\in\mathcal{G}}\left(\binom{w_{bg'}{z_{bg'1},\dots,z_{bg'C}}}\prod_{c'\in\mathcal{C}}p_{g'c'}^{z_{bg'c'}}\right)}$$

Hence, it is possible to approximate the posterior by running as following:

```{r}
mcmc <- run_em(object, method = "mcmc", samples = 1000, step_size = 3000, seed = 42)
mcmc_mae <- mean(abs(mcmc$prob - samples$real_prob))
mcmc_mae
mcmc$time
```

Having a MAE of $0.8\%$, with a running time of $2.58826$ seconds. This method is the most flexible, as it allows for the inclusion of additional parameters. However, it is also one of the slowest, as it requires a large number of samples to achieve a good approximation.

## Exact

Provides the full calculation, using the Law of Total Probabilities:

```{r}
exact <- run_em(object, method = "exact")
exact_mae <- mean(abs(exact$prob - samples$real_prob))
exact_mae
exact$time
```

Providing a MAE of $1.6\%$ with the longest running time.
